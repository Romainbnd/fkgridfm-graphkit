{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridFM Fine-tuning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning an existing GridFM\n",
    "\n",
    "Here we demonstrate how to leverage a previously pre-trained model to perform fine-tuning on downstream tasks. Specifically, we focus on the Power Flow (PF) problem, a fundamental task in power systems that involves computing the steady-state voltages and power injections in the grid.\n",
    "\n",
    "The workflow consists of the following steps:\n",
    "\n",
    "- Similar to pre-training, the first step is to normalize the data and convert the power grid into a PyTorch Geometric graph representation.\n",
    "\n",
    "- A DataLoader then loads the data for fine-tuning.\n",
    "\n",
    "- In the PF use case, which closely aligns with the pre-training setup, we adjust the masking strategy to match the PF problem, i.e. no longer using random masking. For other use cases, it may be necessary to modify the decoder or add additional heads or decoder layers to the pre-trained autoencoder.\n",
    "\n",
    "-  The model is then trained to reconstruct the PF grid state. The loss function consists of a physics-informed loss based on node-wise power balance equations (ensuring power injected equals power consumed or absorbed).  \n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{PBE}} = \\frac{1}{N} \\sum_{i=1}^N \\left| (P_{G,i} - P_{D,i}) + j(Q_{G,i} - Q_{D,i}) - S_{\\text{injection}, i} \\right|\n",
    "$$\n",
    "\n",
    "- Finally, we visualize fine-tuning performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridFM.datasets.powergrid import GridDatasetMem\n",
    "from gridFM.datasets.data_normalization import BaseMVANormalizer\n",
    "from gridFM.training.trainer import Trainer\n",
    "from gridFM.datasets.utils import split_dataset\n",
    "from gridFM.datasets.transforms import AddPFMask\n",
    "from gridFM.training.callbacks import EarlyStopper\n",
    "from gridFM.training.plugins import MetricsTrackerPlugin\n",
    "from gridFM.utils.loss import PBELoss\n",
    "\n",
    "# Standard Libraries\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data and create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select from which grid case file the simulated AC powerflow data should be used\n",
    "data_dir = \"../data/case30_ieee\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_normalizer, edge_normalizer = (\n",
    "    BaseMVANormalizer(node_data=True),\n",
    "    BaseMVANormalizer(node_data=False),\n",
    ")\n",
    "\n",
    "dataset = GridDatasetMem(\n",
    "    root=data_dir,\n",
    "    norm_method=\"baseMVAnorm\",\n",
    "    node_normalizer=node_normalizer,\n",
    "    edge_normalizer=edge_normalizer,\n",
    "    pe_dim=20,  # Dimension of positional encoding\n",
    "    transform=AddPFMask(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_normalizer.to(device)\n",
    "edge_normalizer.to(device)\n",
    "\n",
    "train_dataset, val_dataset, _ = split_dataset(\n",
    "    dataset, data_dir, val_ratio=0.1, test_ratio=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pytorch dataloaders for training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders with batches. The data-Loaders also take care of the masking for the powerflow problem formulation, the masking strategy in the configuration yaml needs to be set to \"pf\".\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\n",
    "    \"../models/GridFM_v0_2_3.pth\", weights_only=False, map_location=device\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select optimizer and learning rate scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.0001,\n",
    ")\n",
    "# Adjust learning rate while training\n",
    "scheduler = ReduceLROnPlateau(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block only for compatibility with original code - does not do anything here\n",
    "best_model_path = os.path.join(\"best_checkpoint.pth\")\n",
    "early_stopper = EarlyStopper(best_model_path, -1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = PBELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plugin logs validation losses and saves to file for later use\n",
    "log_val_loss_plugin = MetricsTrackerPlugin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Trainer Instance -> /gridFM/training/trainer.py\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    loss_fn=loss_fn,\n",
    "    early_stopper=early_stopper,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    lr_scheduler=scheduler,\n",
    "    plugins=[log_val_loss_plugin],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation loss vs. training epochs\n",
    "val_loss = log_val_loss_plugin.get_losses()\n",
    "plt.plot(val_loss)\n",
    "plt.grid()\n",
    "plt.title(\"PF Finetuning Validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
